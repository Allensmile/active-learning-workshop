{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO - Embeddings from Language Models\n",
    "\n",
    "This notebook shows how to use pre-trained ELMO (embeddings from language models) embeddings from the [allennlp](https://github.com/allenai/allennlp) package for featurizing text data. In many cases, ELMO can be used as a drop-in replaced for word vectors such as word2vec, fasttext, or GloVe.\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "- allennlp package\n",
    "- PyTorch \n",
    "\n",
    "We tested this package on the following environment:\n",
    "\n",
    "- Ubuntu 16.04\n",
    "- anaconda python 3.6.6\n",
    "- PyTorch 0.4.0\n",
    "- allenlp 0.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path.home() / \"active-learning-workshop\" / \"text_featurization\" / \"data\"\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-08-15 14:46:28--  https://activelearning.blob.core.windows.net/activelearningdemo/text_data.zip\n",
      "Resolving activelearning.blob.core.windows.net... 13.77.184.64\n",
      "Connecting to activelearning.blob.core.windows.net|13.77.184.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64260679 (61M) [application/x-zip-compressed]\n",
      "Saving to: ‘/home/alizaidi/active-learning-workshop/text_featurization/data/text_data.zip’\n",
      "\n",
      "text_data.zip       100%[===================>]  61.28M  14.3MB/s    in 4.7s    \n",
      "\n",
      "2018-08-15 14:46:33 (13.2 MB/s) - ‘/home/alizaidi/active-learning-workshop/text_featurization/data/text_data.zip’ saved [64260679/64260679]\n",
      "\n",
      "Archive:  /home/alizaidi/active-learning-workshop/text_featurization/data/text_data.zip\n",
      "   creating: /home/alizaidi/active-learning-workshop/text_featurization/data/text_data/\n",
      "  inflating: /home/alizaidi/active-learning-workshop/text_featurization/data/text_data/aggression_data.csv  \n",
      "  inflating: /home/alizaidi/active-learning-workshop/text_featurization/data/text_data/attack_data.csv  \n",
      "  inflating: /home/alizaidi/active-learning-workshop/text_featurization/data/text_data/toxicity_data.csv  \n"
     ]
    }
   ],
   "source": [
    "if not data_dir.joinpath(\"text_data\").exists():\n",
    "    !wget https://activelearning.blob.core.windows.net/activelearningdemo/text_data.zip -P {str(data_dir)}\n",
    "    !unzip {str(data_dir / \"text_data.zip\")} -d {str(data_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"rev_id\",\"comment\",\"year\",\"logged_in\",\"ns\",\"sample\",\"split\",\"count\",\"avg_toxicity\",\"is_toxicity\"\n",
      "2232,\"This:NEWLINE_TOKEN:One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion. NEWLINE_TOKENsounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.NEWLINE_TOKENNEWLINE_TOKENThe paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SRNEWLINE_TOKEN\",2002,\"True\",\"article\",\"random\",\"train\",10,0.4,FALSE\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 ../data/text_data/toxicity_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "toxic_df = pd.read_csv(str(data_dir / \"text_data\" / \"toxicity_data.csv\"), encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = toxic_df.comment.replace(r'NEWLINE_TOKEN|[^.,A-Za-z0-9]+',' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.to_csv(str(data_dir / \"toxic_comments_raw.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This   One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion.  sounds arbitrary and ad hoc. Does it really belong in n encyclopedia article I don t see that it adds anything useful.  The paragraph that follows seems much more useful. Are there any political theorists out there who can clarify the issues It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated... SR \"\n",
      "\"    Clarification for you and Zundark s right, i should have checked the Wikipedia bugs page first . This is a bug in the code that makes wikipedia work it just means that there is a line of code that may have an error as small as an extra space. It s analogous in a VERY simplified way to trying to make something bold in HTML and forgetting to put the at the end, so you d see something like this   words in bold  Instead of this   words in bold   It s not like a virus, that is code somebody deliberately wrote in order to infect your computer and damage files, so it won t go around. JHK    \"\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {str(data_dir / \"toxic_comments_raw.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/jupyter/bin/python: Error while finding module specification for 'allennlp.run' (ImportError: No module named 'allennlp')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "! /anaconda/envs/allennlp/bin/allennlp elmo {str(data_dir / \"toxic_comments_raw.csv\")} {str(data_dir / \"elmo_layers_toxic.csv\")} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/allennlp/bin/python\n"
     ]
    }
   ],
   "source": [
    "!ls {sys.executable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
