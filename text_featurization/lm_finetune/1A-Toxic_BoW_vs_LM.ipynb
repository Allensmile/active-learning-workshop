{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing Toxic Comments Using Pre-Trained Word Vectors and a Language Model's Encoder\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an analysis of featurization methods for text. The central idea we examine is that we can represent text (words, phrases, and even entire sentences or paragraphs) as vectors. However, we'll see that some vector representations may provide more semantic information than others. \n",
    "\n",
    "## Pre-Trained Word Vectors\n",
    "\n",
    "As a first foray into modeling text we featurize our data using publically available pre-trained word vectors. There are numerous pre-trained word vectors available, the most common being [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fasttext](https://github.com/facebookresearch/fastText/). We'll use the **GloVe** vectors trained on Wikipedia and Gigaword 5. \n",
    "\n",
    "Our input dataset contains a variable sequence of tokens (words), which we vectorize into a list of real-valued vectors. In order to use a machine learning model with such a representation we need to transform it into a fixed-vector representation. We can do this by many different aggregation schemes: sum/mean, max, min, etc. For this notebook we simply utilize unweighted averages of all the tokens, but you'll likely find that for some applications it may be more useful to consider max/min in addition, and concatenate multiple representations.\n",
    "\n",
    "\n",
    "![](https://image.slidesharecdn.com/starsem-170916142844/95/yejin-choi-2017-from-naive-physics-to-connotation-modeling-commonsense-in-frame-semantics-83-638.jpg?cb=1505572199)\n",
    "\n",
    "_image credit: Yejin Choi - 2017 - From Naive Physics to Connotation: Modeling Commonsense in Frame Semantics_\n",
    "\n",
    "_quote credit: Ray Mooney_\n",
    "\n",
    "\n",
    "## Language Model Encoders\n",
    "\n",
    "We'll then examine a more advanced method of featurizing our sequence of tokens. In particular, we'll use the encoder from a pre-trained language model. The encoder is a fixed-length vector representation that is typically the last hidden vector in a recurrent neural network trained for machine translation or language modeling.\n",
    "\n",
    "\n",
    "![](http://ruder.io/content/images/2018/07/lm_objective.png)\n",
    "\n",
    "_image credit: Seabstain Ruder and TheGradient: NLP's ImageNet moment has arrived_\n",
    "\n",
    "Our hope is that rather than naively aggregating our word vectors by their average representation, the last hidden layer will contain contextual information from the entire sequence of tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "\n",
    "We import our dataset of comments to Wikipedia page-edits from our helper `load_data`. We'll also import a dictionary of GloVe vectors and a helper function for using it to lookup word vectors for our tokenized comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda/envs/embeddings/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda/envs/embeddings/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda/envs/embeddings/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS=4\n",
    "\n",
    "from load_data import load_wiki_attacks, load_attack_encoded\n",
    "from load_data import tokenize, create_glove_lookup, download_glove\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "data_dir = pathlib.Path(\"/data\") / \"active-learning-workshop\" / \"text_featurization\" / \"data\"\n",
    "\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [spaCy](https://spacy.io/) library to tokenize our text, but aside from some prior data cleanup there's nothing fancy happening in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 822 MB | 7.35 MB/s | 111 sec elapsed"
     ]
    }
   ],
   "source": [
    "glove_src =  str(data_dir / \"glove.6B.300d.txt\")\n",
    "if not pathlib.Path(glove_src).exists():\n",
    "    download_glove(data_dir)\n",
    "glove_lookup = create_glove_lookup(glove_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 61 MB | 7.23 MB/s | 8 sec elapsed"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [  , This, is, not, creative, ., Those, are, t...  \n",
       "1  [    , the, term, standard, model, is, itself,...  \n",
       "2  [  , True, or, false, ,, the, situation, as, o...  \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...  \n",
       "4        [This, page, will, need, disambiguation, .]  \n",
       "5  [    , Important, note, for, all, sysops, Ther...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = load_wiki_attacks(data_dir)\n",
    "toxic_df = tokenize(toxic_df, \"comment_text\")\n",
    "toxic_df.loc[:5, ['comment_text', \"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with GloVe:\n",
    "\n",
    "We can use our `glove_lookup` dictionary to vectorize all the tokens in our text. We apply the function to every token in our comment, and then take the average over all word vectors. Again, you should definitely consider other aggregation methods such as max/min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df['glove_aggregate'] = toxic_df.tokens.apply(lambda x: np.mean([glove_lookup[v] for v in x], axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_aggregate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "      <td>[-0.11586973071617898, 0.05797737403503706, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "      <td>[-0.11100172408932613, 0.09926313644615495, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "      <td>[-0.037980806006074425, -0.029175648491401338,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "      <td>[-0.13784233468778795, 0.10339810541318521, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "      <td>[-0.22114110916778043, -0.07375507116221781, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "      <td>[-0.10041491290032077, 0.07938395149698516, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [  , This, is, not, creative, ., Those, are, t...   \n",
       "1  [    , the, term, standard, model, is, itself,...   \n",
       "2  [  , True, or, false, ,, the, situation, as, o...   \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...   \n",
       "4        [This, page, will, need, disambiguation, .]   \n",
       "5  [    , Important, note, for, all, sysops, Ther...   \n",
       "\n",
       "                                     glove_aggregate  \n",
       "0  [-0.11586973071617898, 0.05797737403503706, 0....  \n",
       "1  [-0.11100172408932613, 0.09926313644615495, 0....  \n",
       "2  [-0.037980806006074425, -0.029175648491401338,...  \n",
       "3  [-0.13784233468778795, 0.10339810541318521, 0....  \n",
       "4  [-0.22114110916778043, -0.07375507116221781, -...  \n",
       "5  [-0.10041491290032077, 0.07938395149698516, -0...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, [\"comment_text\", \"tokens\", \"glove_aggregate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model \n",
    "\n",
    "Our language model encoder utilizes pre-trained language models hosted on [TensorFlow Hub](https://www.tensorflow.org/hub/modules/text). \n",
    "\n",
    "Our helper script `encoder.py` provides a simple class entitled `encoder` with methods for encoding text using three different encoder models: [ELMO](http://www.aclweb.org/anthology/N18-1202), [USE](https://arxiv.org/pdf/1803.11175.pdf), and [NNLM](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Imports:\n",
    "\n",
    "The class is a bit verbose for readability, but it's conceptually very simple. We load the pre-trained module, which defines a static computational graph with the learned weights from the language model on it's dataset. We initialize this computational graph into a Keras session, which we can then use for fine-tuning or for featurizing an input sequence by computing a forward pass of the computational graph. Note, we could have also just used `tensorflow` directly to do the model building and training, but Keras has some helpful utilities for data min-batching and pre-fetching that makes this very easy (at the cost of some incompatibilities: [issues with fine-tuning may arise](https://groups.google.com/a/tensorflow.org/forum/#!topic/hub/Y4AdAM7HpX0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from encoder import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "??encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurized Dataset\n",
    "\n",
    "Here's an example usage of converting the `comment_text` into a fixed sequence using our encoder and the **Universal Sentence Encoder**:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "use_encoder = encoder(model=\"use\")\n",
    "featurizer = use_encoder.transform_model()\n",
    "featurizer.summary()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    transformed_review = featurizer.predict(toxic_df.comment_text.values, batch_size=64)\n",
    "```\n",
    "\n",
    "This operation will take some time, ~1.5 hours on a machine with 16 cores. We have a pre-featurized version of this dataset already saved for you, which you can download using our helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 694 MB | 7.35 MB/s | 94 sec elapsed"
     ]
    }
   ],
   "source": [
    "encoded_attacks = load_attack_encoded(data_dir)\n",
    "toxic_df['encoded_comment'] = encoded_attacks.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>encoded_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[0.026781896, -0.05754256, 0.033774074, 0.0097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[0.011424046, -0.009576778000000001, -0.026437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[0.00041413374, 0.08557465, 0.024096673, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[0.058229163, 0.048170675, -0.054312646, 0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[0.05511004, 0.017830834, -0.08593257, -0.0343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[0.07102389, 0.029018747, -0.04301559, -0.0764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                     encoded_comment  \n",
       "0  [0.026781896, -0.05754256, 0.033774074, 0.0097...  \n",
       "1  [0.011424046, -0.009576778000000001, -0.026437...  \n",
       "2  [0.00041413374, 0.08557465, 0.024096673, -0.09...  \n",
       "3  [0.058229163, 0.048170675, -0.054312646, 0.029...  \n",
       "4  [0.05511004, 0.017830834, -0.08593257, -0.0343...  \n",
       "5  [0.07102389, 0.029018747, -0.04301559, -0.0764...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, ['comment_text', 'encoded_comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "How do these features compare on discrimaniting between toxic / non-toxic comments? Let's put them in a raceoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "train_sizes = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# estimator = GaussianNB()\n",
    "# estimator = RandomForestClassifier()\n",
    "\n",
    "def featurize(df=toxic_df):\n",
    "\n",
    "    labels = np.concatenate(lb.fit_transform(df.is_attack.values))\n",
    "    glove_features = np.vstack(df.glove_aggregate.values)\n",
    "    use_features = np.vstack(df.encoded_comment.values)\n",
    "\n",
    "    return labels, glove_features, use_features\n",
    "\n",
    "labels, glove_features, use_features = featurize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves:\n",
    "\n",
    "Learning curves allows us to visualize the performance of the system as a function of the amount of examples it's seen. We first create learning curves using the `glove_features`, and then we create learning curves of the `encoded_features`. We plot them together so we can compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 0 ns, total: 17.8 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g_train_sizes, g_train_scores, g_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=glove_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=NUM_WORKERS, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 0 ns, total: 29.5 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e_train_sizes, e_train_scores, e_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=use_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=NUM_WORKERS, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We used AUC as our scoring criteria, but you could also use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fda5205fda0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "results_df = pd.DataFrame({\"train_perc\": train_sizes,\n",
    "                           \"bow_auc\": np.mean(g_test_scores, axis=1),\n",
    "                           \"encoder_auc\": np.mean(e_test_scores, axis=1)})\n",
    "\n",
    "sns.lineplot(x=\"train_perc\", y=\"AUC\", hue=\"features\", \n",
    "             data=results_df.melt(\"train_perc\", var_name=\"features\", value_name=\"AUC\"), \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder features outperform the bag-of-words (BoW) glove vectors at every level of training data experience. While a more careful aggregation procedure of the word vectors would have done much better (in fact, there's good evidence that a thoughtful weighted-average can be [very hard to beat](https://openreview.net/forum?id=SyK00v5xx) on many discriminative tasks), the main point of this analysis is that using pre-trained encoders can basically be a drop-in replacement for word vectors for many applications and give significant gains, _modulo_ additional computation time to featurize the dataset: the BoW approach uses a lookup to compute features, which is very fast, whereas the encoder approach requires a full forward pass through a complicated recurrent neural network, which are inherently sequential (this is why there is a greater push towards [feed-forward architectures for language modeling](https://blog.openai.com/language-unsupervised/), which can be much faster during training and evaluation time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis __WARNING: FOUL LANGUAGE AHEAD__:\n",
    "\n",
    "Let's see some examples of where our model using language model features outperformed our BoW model. We'll be a bit more fair to both models this time and do a quick grid search to find a well-optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 1.22 s, total: 31.2 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_df = toxic_df.loc[:,['is_attack', 'comment_text', 'glove_aggregate', 'encoded_comment']]\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "train_df, test_df = train_test_split(model_df, train_size=0.75, random_state=12)\n",
    "\n",
    "def cv_predict_eval():\n",
    "\n",
    "#     cv = GridSearchCV(RandomForestClassifier(),\n",
    "#                      param_grid={\n",
    "#                          'n_estimators': [10, 100],\n",
    "#                          'max_features': ['sqrt', 'log2'],\n",
    "#                          'max_depth': [3, 5, None]\n",
    "#                      }, \n",
    "#                       refit=True, \n",
    "#                       n_jobs=NUM_WORKERS)\n",
    "\n",
    "    cv = LogisticRegression()\n",
    "\n",
    "    labels, glove_features, use_features = featurize(train_df)\n",
    "    labels_test, glove_test, use_test = featurize(test_df)\n",
    "\n",
    "    glove_fit = cv.fit(glove_features, labels)\n",
    "    glove_hat = glove_fit.predict(glove_test)\n",
    "    use_fit = cv.fit(use_features, labels)\n",
    "    use_hat = use_fit.predict(use_test)\n",
    "\n",
    "    results_df = test_df\n",
    "    results_df.loc[:, 'use_pred'] = use_hat\n",
    "    results_df.loc[:, 'glove_pred'] = glove_hat\n",
    "\n",
    "    return results_df\n",
    "\n",
    "results_df = cv_predict_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where BoW-GloVe Fails and the Encoder Succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105512</th>\n",
       "      <td>thanks a lot man, you dont know how long thats...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77299</th>\n",
       "      <td>im half crazy figure out which side...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86232</th>\n",
       "      <td>Archive Indexerbot can blank this</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87038</th>\n",
       "      <td>stop with the warnings ill just be back lol...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65516</th>\n",
       "      <td>Understood. Thanks.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75616</th>\n",
       "      <td>I REALLY DONT GIVE TWO ABOUT BEING BANNED  ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69583</th>\n",
       "      <td>NO I HATE IT BECAUSE YOU DELETE EVERYTHING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31237</th>\n",
       "      <td>Longest Monosyllabic   Squirrelled, anybody...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24483</th>\n",
       "      <td>Edit to Benito Ju rez</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60775</th>\n",
       "      <td>Canadian  Why is he listed as a Canadian Am...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>Man let me edit the damn Pink Spiders page.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96188</th>\n",
       "      <td>Category Wikipedia sockpuppets of TungstenCa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60535</th>\n",
       "      <td>ha ha ha . ha ha ha ha ha . ha ha ha haha a h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10523</th>\n",
       "      <td>Comment James, get over yourself.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44206</th>\n",
       "      <td>mamas boy mamas boy mamas boy mamas boymamas b...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88354</th>\n",
       "      <td>calm your self boy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49585</th>\n",
       "      <td>Obsessive compulsive disorder</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46352</th>\n",
       "      <td>Weasel Words Everywhere They haunt me</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98134</th>\n",
       "      <td>Just plain Bill Just plain Bill</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108311</th>\n",
       "      <td>gonna do about it  Yeah ee yeah, yeah ee  Shak...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82827</th>\n",
       "      <td>Bud, do simran and don t bother me again.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73311</th>\n",
       "      <td>You could do worse.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50225</th>\n",
       "      <td>hello clown  Rust never sleeps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70627</th>\n",
       "      <td>lol wtf i aint vandalising seriously i tried n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>Sure, but then you stalked me</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13540</th>\n",
       "      <td>No sweat dude. I probably deserved it...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57827</th>\n",
       "      <td>jews  the only people that do conspiring is...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47355</th>\n",
       "      <td>hey wats up people</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18736</th>\n",
       "      <td>Strong disagree Just because a poodle is a d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57039</th>\n",
       "      <td>THis is terrible   Just rewrite the whole t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78311</th>\n",
       "      <td>Can you delete this vandalism</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36123</th>\n",
       "      <td>VideoClip2 SalsaBalroomDance Country TBA Semi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95626</th>\n",
       "      <td>Did you go to AussieLegend s talk page to g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19895</th>\n",
       "      <td>If stone can bleed blood, we don t need the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51840</th>\n",
       "      <td>speakinBeff and lorna herehig</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44719</th>\n",
       "      <td>CAN T HAVE MY POPCORN FRO</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>YOU STILL REFUSE TO DO ANYTHING ABOUT THE V...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>This is unhelpful. You wish a unilateral ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31720</th>\n",
       "      <td>like Naruto and Shikamaru. And I can dig a ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68817</th>\n",
       "      <td>Another administator impersonator.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54537</th>\n",
       "      <td>Smile        Smile    Smile    Smile    Smile...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84901</th>\n",
       "      <td>Try this one http en.wikipedia.org wiki File...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35430</th>\n",
       "      <td>why do you keep reverting my edits</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28687</th>\n",
       "      <td>you do know that you are a dupe, right just ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46534</th>\n",
       "      <td>I am tired of his insults.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40243</th>\n",
       "      <td>. AND NO I HAVE NEVER USED AND OR CREATED ANOT...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64479</th>\n",
       "      <td>Realpolitik and the North German Confederation</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30138</th>\n",
       "      <td>Yup. It s pointless and it s gone.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68390</th>\n",
       "      <td>so stop reverting the damn deletion</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93038</th>\n",
       "      <td>High Gross Revenue   How the hell is Contin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>Thanks  Thanks for cleaning up vandalism on...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>ANYTHING YOU SAY CAN AND WILL BE USED AGAINST...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69665</th>\n",
       "      <td>Mokele what is the largest black caiman you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29758</th>\n",
       "      <td>DAVID BECKHAM IS BUFF</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97642</th>\n",
       "      <td>and Germans redirets to German people</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64868</th>\n",
       "      <td>I just did some calculations. Let P N be the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59456</th>\n",
       "      <td>i am sorry for vandalising i promise not to do...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88181</th>\n",
       "      <td>UPDATED WITH REFERENCES NOW</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102777</th>\n",
       "      <td>Are you a racist Do you have something against...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16627</th>\n",
       "      <td>for a lying phony, Tijuana Brass</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  is_attack\n",
       "105512  thanks a lot man, you dont know how long thats...      False\n",
       "77299              im half crazy figure out which side...      False\n",
       "86232                  Archive Indexerbot can blank this       False\n",
       "87038      stop with the warnings ill just be back lol...      False\n",
       "65516                                Understood. Thanks.       False\n",
       "75616      I REALLY DONT GIVE TWO ABOUT BEING BANNED  ...      False\n",
       "69583          NO I HATE IT BECAUSE YOU DELETE EVERYTHING      False\n",
       "31237      Longest Monosyllabic   Squirrelled, anybody...      False\n",
       "24483                            Edit to Benito Ju rez         False\n",
       "60775      Canadian  Why is he listed as a Canadian Am...      False\n",
       "21585         Man let me edit the damn Pink Spiders page.      False\n",
       "96188     Category Wikipedia sockpuppets of TungstenCa...      False\n",
       "60535    ha ha ha . ha ha ha ha ha . ha ha ha haha a h...      False\n",
       "10523                Comment James, get over yourself.         False\n",
       "44206   mamas boy mamas boy mamas boy mamas boymamas b...      False\n",
       "88354                                  calm your self boy      False\n",
       "49585                    Obsessive compulsive disorder         False\n",
       "46352             Weasel Words Everywhere They haunt me        False\n",
       "98134                     Just plain Bill Just plain Bill      False\n",
       "108311  gonna do about it  Yeah ee yeah, yeah ee  Shak...      False\n",
       "82827           Bud, do simran and don t bother me again.      False\n",
       "73311                                 You could do worse.      False\n",
       "50225                      hello clown  Rust never sleeps      False\n",
       "70627   lol wtf i aint vandalising seriously i tried n...      False\n",
       "69877                      Sure, but then you stalked me       False\n",
       "13540           No sweat dude. I probably deserved it...       False\n",
       "57827      jews  the only people that do conspiring is...      False\n",
       "47355                              hey wats up people          False\n",
       "18736     Strong disagree Just because a poodle is a d...      False\n",
       "57039      THis is terrible   Just rewrite the whole t...      False\n",
       "...                                                   ...        ...\n",
       "78311                      Can you delete this vandalism       False\n",
       "36123    VideoClip2 SalsaBalroomDance Country TBA Semi...      False\n",
       "95626      Did you go to AussieLegend s talk page to g...      False\n",
       "19895     If stone can bleed blood, we don t need the ...      False\n",
       "51840                       speakinBeff and lorna herehig      False\n",
       "44719                           CAN T HAVE MY POPCORN FRO      False\n",
       "8307       YOU STILL REFUSE TO DO ANYTHING ABOUT THE V...      False\n",
       "1553       This is unhelpful. You wish a unilateral ri...      False\n",
       "31720    like Naruto and Shikamaru. And I can dig a ho...      False\n",
       "68817                Another administator impersonator.        False\n",
       "54537    Smile        Smile    Smile    Smile    Smile...      False\n",
       "84901     Try this one http en.wikipedia.org wiki File...      False\n",
       "35430                why do you keep reverting my edits        False\n",
       "28687     you do know that you are a dupe, right just ...      False\n",
       "46534                        I am tired of his insults.        False\n",
       "40243   . AND NO I HAVE NEVER USED AND OR CREATED ANOT...      False\n",
       "64479      Realpolitik and the North German Confederation      False\n",
       "30138                  Yup. It s pointless and it s gone.      False\n",
       "68390               so stop reverting the damn deletion        False\n",
       "93038      High Gross Revenue   How the hell is Contin...      False\n",
       "9774       Thanks  Thanks for cleaning up vandalism on...      False\n",
       "3032     ANYTHING YOU SAY CAN AND WILL BE USED AGAINST...      False\n",
       "69665     Mokele what is the largest black caiman you ...      False\n",
       "29758                              DAVID BECKHAM IS BUFF       False\n",
       "97642               and Germans redirets to German people      False\n",
       "64868     I just did some calculations. Let P N be the...      False\n",
       "59456   i am sorry for vandalising i promise not to do...      False\n",
       "88181                        UPDATED WITH REFERENCES NOW       False\n",
       "102777  Are you a racist Do you have something against...      False\n",
       "16627                 for a lying phony, Tijuana Brass         False\n",
       "\n",
       "[443 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[(results_df[\"is_attack\"] == results_df[\"use_pred\"]) & \n",
    "               (results_df[\"is_attack\"] != results_df[\"glove_pred\"]) & \n",
    "               (results_df[\"is_attack\"] == False), \n",
    "               [\"comment_text\", \"is_attack\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i am sorry for vandalising i promise not to do it anymore'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[[59456], \"comment_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thanks a lot man, you dont know how long thats been fuckin with my head  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[[105512], \"comment_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unfortunately pretty challenging to find any civil discussion on online forums. In the example highlighted above our BoW approach predicted the comment to be an attack, whereas the encoder correctly predicted it as benign. The prevalence of terms such \"dumb\", \"sorry\" _bias_ the average word vector to a representation that is more likely to be an attack than a nice comment, whereas the language model is able to encode the sequence of representations more accurately. A more acute example of this phenomena arises with swear words: for example record index `[105512]` for an example where the single occurrence of a curse word causes the BoW classifier to misclassify the sentiment dramatically (but the encoder gets it right). This also arises frequently in the use of **negation** in sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
