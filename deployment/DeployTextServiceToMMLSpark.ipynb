{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 0.1.0.1005480\n"
     ]
    }
   ],
   "source": [
    "# install all the required dependencies\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files here\n"
     ]
    }
   ],
   "source": [
    "# get the text data from the github repo and unzip it\n",
    "from fit_and_store_pipeline import unzip_file_here\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('./text_data/attack_data.csv'):\n",
    "    if not os.path.isfile('./text_data.zip'): \n",
    "        urllib.request.urlretrieve('https://activelearning.blob.core.windows.net/activelearningdemo/text_data.zip', 'text_data.zip')\n",
    "    unzip_file_here('text_data.zip')\n",
    "\n",
    "if not os.path.isfile('miniglove_6B_50d_w2v.txt'):\n",
    "    unzip_file_here('miniglove_6B_50d_w2v.zip')\n",
    "    \n",
    "print('Data files here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pipeline_parts import *\n",
    "\n",
    "# text preprocessor\n",
    "preprocessor = GensimPreprocessor()\n",
    "\n",
    "# word2vec featurizer\n",
    "w2v_file = 'miniglove_6B_50d_w2v.txt' # convert glove file to w2v format using gensim.scripts.glove2word2vec\n",
    "word_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=False)\n",
    "vectorizer = AvgWordVectorFeaturizer(word_vectors)\n",
    "\n",
    "# classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=1)\n",
    "\n",
    "# assemble a scikit-learn pipeline\n",
    "model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', classifier),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a train-test data pair\n",
    "\n",
    "from fit_and_store_pipeline import create_train_test_split\n",
    "\n",
    "# requires training_set_01.csv and test_set_01.csv to be present\n",
    "training_data, test_data = create_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', GensimPreprocessor(newline_token=None)), ('vectorizer', <pipeline_parts.AvgWordVectorFeaturizer object at 0x7ff6a0901a90>), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nod...estimators=100, n_jobs=1,\n",
       "            oob_score=True, random_state=1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test - does the pipeline fit happily in a scikit fashion?\n",
    "fitted_pipe = model_pipeline.fit(training_data.comment, [int(x) for x in training_data.is_attack])\n",
    "fitted_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction from the scikit pipeline\n",
    "test_attacks = ['You are scum.', 'I like your shoes.', 'You are pxzx.', \n",
    "             'Your mother was a hamster and your father smelt of elderberries',\n",
    "             'One bag of hagfish slime, please']\n",
    "\n",
    "fitted_pipe.predict(test_attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as a Spark Streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plan:\n",
    "#\n",
    "# 1. Collect the training data in a spark dataframe\n",
    "# 2. Build a model (pipeline duck-typed like the model in notebook 108)\n",
    "# 3. Use TrainClassifier to train it\n",
    "# 4. Use SparkStreaming to make a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[comment: string, year: bigint, logged_in: boolean, ns: string, sample: string, split: string, count: bigint, avg_attack: double, is_attack: boolean]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn the training data from a pandas.DataFrame into a spark.DataFrame\n",
    "# type(training_data)\n",
    "# training_data\n",
    "train_sdf = spark.createDataFrame(training_data)\n",
    "train_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the same pipeline using the MMLSpark TrainClassifier (Will not work)\n",
    "\n",
    "from mmlspark import TrainClassifier\n",
    "model = TrainClassifier(model=fitted_pipe, labelCol=\"is_attack\", numFeatures=256).fit(train_sdf)\n",
    "\n",
    "# we cannot use a scikit pipeline in here:\n",
    "# 'Pipeline' object has no attribute '_to_java'\n",
    "# It looks like only native classifiers are OK\n",
    "#\n",
    "# The new game plan is to run the scikit pipeline for featurization,\n",
    "# then feed the featurized data into a native MML Spark classifier for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     comment  year  logged_in  \\\n",
      "rev_id                                                                          \n",
      "416170166  NEWLINE_TOKENNEWLINE_TOKEN== Please stop? ==NE...  2011      False   \n",
      "180046743  NEWLINE_TOKENfine I get threatened and he is a...  2007       True   \n",
      "536943251  Lets see double standard of some one who tries...  2013       True   \n",
      "93070973   my comments in on the arbitration request he f...  2006       True   \n",
      "272666818  `NEWLINE_TOKENNEWLINE_TOKEN==You maybe interes...  2009       True   \n",
      "\n",
      "                ns   sample  split  count  avg_attack  is_attack  \n",
      "rev_id                                                            \n",
      "416170166     user  blocked  train      8    0.125000      False  \n",
      "180046743     user  blocked  train      9    0.222222      False  \n",
      "536943251  article  blocked    dev     10    0.200000      False  \n",
      "93070973      user  blocked   test      9    0.000000      False  \n",
      "272666818     user   random  train     10    0.000000      False  \n",
      "         F0        F1        F2        F3        F4        F5        F6  \\\n",
      "0 -0.182730  0.314910  0.578810 -0.451117  0.138780 -0.619488 -0.023427   \n",
      "1  0.097344  0.116066 -0.129499 -0.183653  0.444738  0.039917 -0.334430   \n",
      "2  0.349084  0.100745 -0.045672 -0.082555  0.361282  0.197562 -0.288936   \n",
      "3  0.147019  0.124288 -0.327029 -0.114524  0.330564  0.050204 -0.517314   \n",
      "4  0.308448  0.147917  0.065566 -0.116795  0.349226  0.003178 -0.419646   \n",
      "\n",
      "         F7        F8        F9    ...           F41       F42       F43  \\\n",
      "0  0.537403  0.039993 -0.349530    ...      0.191119  0.263763  0.151110   \n",
      "1  0.123569 -0.113374  0.226857    ...      0.003958  0.023422  0.221236   \n",
      "2 -0.192525 -0.094679  0.063190    ...      0.079737  0.052697  0.193157   \n",
      "3  0.237108 -0.212837 -0.163897    ...      0.156871 -0.208901  0.238045   \n",
      "4 -0.121481 -0.071388 -0.198658    ...     -0.170733  0.106964  0.088627   \n",
      "\n",
      "        F44       F45       F46       F47       F48       F49  is_attack  \n",
      "0  0.493073  0.359977  0.113137  0.025230  0.238686  0.728423          0  \n",
      "1 -0.099966 -0.097279 -0.299032 -0.088193  0.067070  0.082129          0  \n",
      "2  0.031384  0.035903 -0.199008  0.127317  0.035283 -0.024386          0  \n",
      "3 -0.185906  0.023333 -0.140447  0.142880 -0.089312 -0.255789          0  \n",
      "4  0.036544  0.094055 -0.127313  0.134295 -0.051470  0.079612          0  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[F0: double, F1: double, F2: double, F3: double, F4: double, F5: double, F6: double, F7: double, F8: double, F9: double, F10: double, F11: double, F12: double, F13: double, F14: double, F15: double, F16: double, F17: double, F18: double, F19: double, F20: double, F21: double, F22: double, F23: double, F24: double, F25: double, F26: double, F27: double, F28: double, F29: double, F30: double, F31: double, F32: double, F33: double, F34: double, F35: double, F36: double, F37: double, F38: double, F39: double, F40: double, F41: double, F42: double, F43: double, F44: double, F45: double, F46: double, F47: double, F48: double, F49: double, is_attack: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforms-only pipeline\n",
    "trans_pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('vectorizer', vectorizer)\n",
    "    ])\n",
    "\n",
    "trans_pipe.fit(training_data.comment, [int(x) for x in training_data.is_attack])\n",
    "featurized_train = trans_pipe.transform(training_data.comment)\n",
    "feature_names = [ 'F' + str(i) for i in range(featurized_train.shape[1]) ]\n",
    "\n",
    "ft_df = pd.DataFrame(featurized_train, columns=feature_names);\n",
    "ft_df['is_attack'] = [1 if x else 0 for x in training_data['is_attack']]\n",
    "\n",
    "print(training_data.head())\n",
    "print(ft_df.head())\n",
    "\n",
    "featurized_train_sdf = spark.createDataFrame(ft_df)\n",
    "featurized_train_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import TrainClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "model = TrainClassifier(model=LogisticRegression(), \n",
    "                        labelCol=\"is_attack\", \n",
    "                        numFeatures=256).fit(featurized_train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- F0: double (nullable = true)\n",
      " |-- F1: double (nullable = true)\n",
      " |-- F2: double (nullable = true)\n",
      " |-- F3: double (nullable = true)\n",
      " |-- F4: double (nullable = true)\n",
      " |-- F5: double (nullable = true)\n",
      " |-- F6: double (nullable = true)\n",
      " |-- F7: double (nullable = true)\n",
      " |-- F8: double (nullable = true)\n",
      " |-- F9: double (nullable = true)\n",
      " |-- F10: double (nullable = true)\n",
      " |-- F11: double (nullable = true)\n",
      " |-- F12: double (nullable = true)\n",
      " |-- F13: double (nullable = true)\n",
      " |-- F14: double (nullable = true)\n",
      " |-- F15: double (nullable = true)\n",
      " |-- F16: double (nullable = true)\n",
      " |-- F17: double (nullable = true)\n",
      " |-- F18: double (nullable = true)\n",
      " |-- F19: double (nullable = true)\n",
      " |-- F20: double (nullable = true)\n",
      " |-- F21: double (nullable = true)\n",
      " |-- F22: double (nullable = true)\n",
      " |-- F23: double (nullable = true)\n",
      " |-- F24: double (nullable = true)\n",
      " |-- F25: double (nullable = true)\n",
      " |-- F26: double (nullable = true)\n",
      " |-- F27: double (nullable = true)\n",
      " |-- F28: double (nullable = true)\n",
      " |-- F29: double (nullable = true)\n",
      " |-- F30: double (nullable = true)\n",
      " |-- F31: double (nullable = true)\n",
      " |-- F32: double (nullable = true)\n",
      " |-- F33: double (nullable = true)\n",
      " |-- F34: double (nullable = true)\n",
      " |-- F35: double (nullable = true)\n",
      " |-- F36: double (nullable = true)\n",
      " |-- F37: double (nullable = true)\n",
      " |-- F38: double (nullable = true)\n",
      " |-- F39: double (nullable = true)\n",
      " |-- F40: double (nullable = true)\n",
      " |-- F41: double (nullable = true)\n",
      " |-- F42: double (nullable = true)\n",
      " |-- F43: double (nullable = true)\n",
      " |-- F44: double (nullable = true)\n",
      " |-- F45: double (nullable = true)\n",
      " |-- F46: double (nullable = true)\n",
      " |-- F47: double (nullable = true)\n",
      " |-- F48: double (nullable = true)\n",
      " |-- F49: double (nullable = true)\n",
      " |-- is_attack: long (nullable = true)\n",
      " |-- scores: vector (nullable = true)\n",
      " |-- scored_probabilities: vector (nullable = true)\n",
      " |-- scored_labels: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "# first, featurize the test data using the same pipeline\n",
    "featurized_test = trans_pipe.transform(test_data.comment)\n",
    "# use the same feature names we've had\n",
    "# feature_names = [ 'F' + str(i) for i in range(featurized_test.shape[1]) ]\n",
    "\n",
    "ftst_df = pd.DataFrame(featurized_test, columns=feature_names);\n",
    "ftst_df['is_attack'] = [1 if x else 0 for x in test_data['is_attack']]\n",
    "\n",
    "# second, make a prediction on the test set\n",
    "ftst_sdf = spark.createDataFrame(ft_df)\n",
    "\n",
    "prediction = model.transform(ftst_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_type</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification</td>\n",
       "      <td>DenseMatrix([[ 250.,   32.],\\n             [  ...</td>\n",
       "      <td>0.779545</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.588608</td>\n",
       "      <td>0.847675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  evaluation_type                                   confusion_matrix  \\\n",
       "0  Classification  DenseMatrix([[ 250.,   32.],\\n             [  ...   \n",
       "\n",
       "   accuracy  precision    recall       AUC  \n",
       "0  0.779545      0.744  0.588608  0.847675  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third, compute performance stats\n",
    "from mmlspark import ComputeModelStatistics, TrainedClassifierModel\n",
    "metrics = ComputeModelStatistics().transform(prediction)\n",
    "metrics.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now deploy the trained classifier as a streaming job\n",
    "# define the interface to be like the model's input\n",
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "\n",
    "serving_inputs = spark.readStream.server() \\\n",
    "    .address(\"localhost\", 9999, \"text_api\") \\\n",
    "    .load()\\\n",
    "    .withColumn(\"variables\", from_json(col(\"value\"), ftst_sdf.schema))\\\n",
    "    .select(\"id\",\"variables.*\")\n",
    "\n",
    "# says to extract \"variables\" from the \"value\" field of json-encoded webservice input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serving_outputs = model.transform(serving_inputs) \\\n",
    "  .withColumn(\"scored_labels\", col(\"scored_labels\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = serving_outputs.writeStream \\\n",
    "    .server() \\\n",
    "    .option(\"name\", \"text_api\") \\\n",
    "    .queryName(\"my_query\") \\\n",
    "    .option(\"replyCol\", \"scored_labels\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints-{}\".format(uuid.uuid1())) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if we want to change something above, we'll need\n",
    "# to stop the active server\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are scum.', 'I like your shoes.', 'You are pxzx.', 'Your mother was a hamster and your father smelt of elderberries', 'One bag of hagfish slime, please']\n",
      "{'F0': -0.21418767, 'F1': 0.40059671, 'F2': 0.0071733347, 'F3': -0.33567235, 'F4': 0.50522667, 'F5': 0.080480002, 'F6': -0.45744696, 'F7': -0.746481, 'F8': 0.26008001, 'F9': 0.071898997, 'F10': -0.098540008, 'F11': 0.65155435, 'F12': -0.36874136, 'F13': 0.16127668, 'F14': 0.53228664, 'F15': 0.55447334, 'F16': -0.29821333, 'F17': 0.42311099, 'F18': 0.43166566, 'F19': -1.27487, 'F20': -0.058775995, 'F21': 0.48372331, 'F22': 0.13969998, 'F23': 0.076476656, 'F24': 0.085326649, 'F25': -1.6213666, 'F26': -1.1134467, 'F27': 0.58301669, 'F28': 0.98441666, 'F29': -1.1452667, 'F30': 2.8082666, 'F31': 0.97427672, 'F32': -0.29472566, 'F33': 0.48504066, 'F34': -0.058868002, 'F35': 0.60678333, 'F36': -0.069305666, 'F37': 0.26238334, 'F38': 0.23104002, 'F39': -0.42316332, 'F40': 0.21481667, 'F41': 0.27335998, 'F42': 0.044187605, 'F43': 0.61054331, 'F44': 0.6356377, 'F45': -0.12235234, 'F46': 0.14373334, 'F47': -0.78377336, 'F48': -0.10396666, 'F49': 0.12181666}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'float32' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-55d39a5348ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrow_as_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_as_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mws_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_as_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/lib/conda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'float32' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "print(test_attacks)\n",
    "data = pd.DataFrame(trans_pipe.transform(test_attacks), columns=feature_names)\n",
    "# turn data into a json string representation of a dictionary\n",
    "row_as_dict = data.to_dict('records')[1]\n",
    "print(row_as_dict)\n",
    "ws_input = json.dumps(row_as_dict)\n",
    "\n",
    "print(ws_input)\n",
    "\n",
    "r = requests.post(data=ws_input, url=\"http://localhost:9999/text_api\")\n",
    "print(\"Response {}\".format(r.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed 100 rows of data to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "test_samples = json.dumps({\"data\": vectors.tolist()})\n",
    "test_samples = bytes(test_samples, encoding = 'utf8')\n",
    "\n",
    "# Call scoring service\n",
    "service.run(input_data = test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting ACI is super fast!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
