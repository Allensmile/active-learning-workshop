{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the featurizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GensimPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, newline_token='NEWLINE_TOKEN'):\n",
    "        self.newline_pat = re.compile(newline_token)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [ list(self.tokenize(txt)) for txt in X ]\n",
    "    \n",
    "    def tokenize(self, doc):\n",
    "        doc = self.newline_pat.sub(' ', doc)\n",
    "        return gensim.utils.simple_preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgWordVectorFeaturizer(object):\n",
    "    def __init__(self, embedding, restrict_vocab=400000):\n",
    "        self.embedding = embedding\n",
    "        self.word2index = { w:i for i,w in enumerate(embedding.index2word) }\n",
    "        self.restrict_vocab = restrict_vocab\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X is a list of tokenized documents\n",
    "        return np.array([\n",
    "            np.mean([self.embedding[t] for t in token_vec \n",
    "                        if t in self.embedding and \n",
    "                        (self.word2index[t] < self.restrict_vocab)\n",
    "                    ]\n",
    "                    or [np.zeros(self.embedding.vector_size)], axis=0)\n",
    "            for token_vec in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split, preprocess, and featurize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data files\n",
    "\n",
    "w2v_file = 'miniglove_6B_50d_w2v.txt'\n",
    "text_data_file = \"attack_data.csv\"\n",
    "training_set_file = \"training_set_01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=False)\n",
    "\n",
    "text_data = pd.read_csv(text_data_file, encoding='windows-1252')\n",
    "\n",
    "text_data = text_data.set_index(\"rev_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IDs of the training set and candidate test set observations\n",
    "\n",
    "training_set_rev_ids = pd.read_csv(training_set_file).rev_id\n",
    "\n",
    "test_candidate_rev_ids = set.difference(set(text_data.index.values), set(training_set_rev_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of the candidate test set observations\n",
    "\n",
    "random.seed(1)\n",
    "test_set_rev_ids = random.sample(test_candidate_rev_ids, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the training and test set into separate Pandas dataframes\n",
    "\n",
    "training_data = text_data.loc[training_set_rev_ids]\n",
    "test_data = text_data.loc[test_set_rev_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate label lists\n",
    "\n",
    "labels = [int(x) for x in training_data.is_attack]\n",
    "test_labels = [int(x) for x in test_data.is_attack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the featurizers\n",
    "\n",
    "gp = GensimPreprocessor()\n",
    "featurizer = AvgWordVectorFeaturizer(embedding=word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize the training data\n",
    "\n",
    "preprocessed_data = gp.transform(training_data.comment)\n",
    "featurized_data = featurizer.transform(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize the test data\n",
    "\n",
    "preprocessed_test_data = gp.transform(test_data.comment)\n",
    "featurized_test_data = featurizer.transform(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train default (un-tuned) scikit-learn random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.2 ms, sys: 108 Âµs, total: 30.3 ms\n",
      "Wall time: 28.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "fitted_model = classifier_model.fit(featurized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8026801186123085\n"
     ]
    }
   ],
   "source": [
    "pred_prob = fitted_model.predict_proba(featurized_test_data)\n",
    "\n",
    "scores = pred_prob[:,1]\n",
    "\n",
    "auc = roc_auc_score(test_labels, scores)\n",
    "\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training set features and labels\n",
    "\n",
    "featurized_data_df = pd.DataFrame(featurized_data).reset_index(drop=True)\n",
    "\n",
    "labels_df = pd.DataFrame(labels).rename(index=str, columns={0: \"Label\"}).reset_index(drop=True)\n",
    "\n",
    "labeled_data = pd.concat([labels_df, featurized_data_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test set features and labels\n",
    "\n",
    "featurized_test_data_df = pd.DataFrame(featurized_test_data).reset_index(drop=True)\n",
    "\n",
    "test_labels_df = pd.DataFrame(test_labels).rename(index=str, columns={0: \"Label\"}).reset_index(drop=True)\n",
    "\n",
    "labeled_test_data = pd.concat([test_labels_df, featurized_test_data_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of PySpark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import mmlspark\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "import os, urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark dataframes\n",
    "\n",
    "tune = spark.createDataFrame(labeled_data)\n",
    "test = spark.createDataFrame(labeled_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune logistic regression, random forest, and GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import TuneHyperparameters\n",
    "from mmlspark.TrainClassifier import TrainClassifier\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "# Define the models to try: Logistic Regression, Random Forest, and Gradient Boosted Trees\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "randForest = RandomForestClassifier()\n",
    "gbt = GBTClassifier()\n",
    "\n",
    "smlmodels = [logReg, randForest, gbt]\n",
    "\n",
    "mmlmodels = [TrainClassifier(model=model, labelCol=\"Label\") for model in smlmodels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import HyperparamBuilder\n",
    "from mmlspark import RangeHyperParam\n",
    "from mmlspark import DiscreteHyperParam\n",
    "from mmlspark import RandomSpace\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "\n",
    "paramBuilder = \\\n",
    "  HyperparamBuilder() \\\n",
    "    .addHyperparam(logReg, logReg.regParam, RangeHyperParam(0.1, 0.3, isDouble=True)) \\\n",
    "    .addHyperparam(randForest, randForest.numTrees, RangeHyperParam(50, 1000)) \\\n",
    "    .addHyperparam(randForest, randForest.maxDepth, RangeHyperParam(3, 30)) \\\n",
    "    .addHyperparam(randForest, randForest.maxBins, RangeHyperParam(100, 1000)) \\\n",
    "    .addHyperparam(randForest, randForest.impurity, DiscreteHyperParam(['gini', 'entropy'])) \\\n",
    "    .addHyperparam(gbt, gbt.maxBins, RangeHyperParam(100, 1000)) \\\n",
    "    .addHyperparam(gbt, gbt.maxDepth, RangeHyperParam(3, 30))\n",
    "\n",
    "randomSpace = RandomSpace(paramBuilder.build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 409 ms, sys: 245 ms, total: 654 ms\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bestModel = TuneHyperparameters(\n",
    "              evaluationMetric=\"AUC\", models=mmlmodels, numFolds=3,\n",
    "              numRuns=len(mmlmodels) * 1, parallelism=2,\n",
    "              paramSpace=randomSpace.space(), seed=0).fit(tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: false\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: auto\n",
      "featuresCol: TrainClassifier_4d17a05267268ba671a7_features\n",
      "impurity: entropy\n",
      "labelCol: Label\n",
      "maxBins: 548\n",
      "maxDepth: 28\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "numTrees: 948\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: 3385818979260681125\n",
      "subsamplingRate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters of the best model\n",
    "\n",
    "bestModelInfo = bestModel._java_obj.getBestModelInfo()\n",
    "\n",
    "for entry in bestModelInfo.split(', '):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the best model on the held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>evaluation_type</th>\n",
       "      <td>Classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_0.0_actual_is_0.0</th>\n",
       "      <td>8569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_0.0_actual_is_1.0</th>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_1.0_actual_is_0.0</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_1.0_actual_is_1.0</th>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.9046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.751181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.374705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.88117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \n",
       "evaluation_type                       Classification\n",
       "predicted_class_as_0.0_actual_is_0.0            8569\n",
       "predicted_class_as_0.0_actual_is_1.0             796\n",
       "predicted_class_as_1.0_actual_is_0.0             158\n",
       "predicted_class_as_1.0_actual_is_1.0             477\n",
       "accuracy                                      0.9046\n",
       "precision                                   0.751181\n",
       "recall                                      0.374705\n",
       "AUC                                          0.88117"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmlspark import ComputeModelStatistics\n",
    "\n",
    "prediction = bestModel.transform(test)\n",
    "\n",
    "metrics = ComputeModelStatistics().transform(prediction)\n",
    "\n",
    "metrics.toPandas().transpose().rename(columns={0: ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
